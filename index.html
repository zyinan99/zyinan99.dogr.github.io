<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Towards Versatile Visual Document Grounding and Referring">
  <meta name="keywords" content="DOGE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DOGE: Towards Versatile Visual Document Grounding and Referring</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/doge.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="./static/images/doge.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span class="doge" style="vertical-align: middle">DOGE</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Towards Versatile Visual Document Grounding and Referring
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yinan Zhou*<sup style="color:#6fbf73;">1</sup><sup style="color:#000000;">,</sup><sup style="color:#ed4b82;">2</sup><sup style="color:#000000;">,</sup><sup style="color:#9400D3">3</sup>,</span>
            <span class="author-block">
              Yuxin Chen*‚Ä†<sup style="color:#ed4b82;">2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/FelixMessi">Haokun Lin</a><sup style="color:#ed4b82">2</sup><sup style="color:#000000;">,</sup><sup style="color:#9400D3">3</sup><sup style="color:#000000;">,</sup><sup style="color:#ffac33">4</sup>,
            </span>
            <span class="author-block">
              Shuyu Yang<sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              Li Zhu<sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              Zhongang Qi‚Ä°<sup style="color:#ed4b82;">2</sup>,
            </span>
            <span class="author-block">
              Chen Ma‚Ä°<sup style="color:#9400D3">3</sup>,
            </span>
            <span class="author-block">
              Ying Shan<sup style="color:#ed4b82;">2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"></span>*Equal Contribution ‚Ä†Project Lead ‚Ä°Corresponding Authors ,</span><br>
            <span class="author-block"><sup style="color:#6fbf73;">1</sup>Xi‚Äôan Jiaotong University,</span>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>ARC Lab, Tencent PCG,</span>
            <span class="author-block"><sup style="color:#9400D3">3</sup>City University of Hongkong,</span><br>
            <span class="author-block"><sup style="color:#ffac33">4</sup>Institute of Automation, CAS</span><br>
          </div>
        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- @PAN TODO: change links -->
                <a href="https://arxiv.org/pdf/2411.17125"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.17125"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zyinan99/DOGE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/zyinan99/DOGE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span> 
              <!-- Visualization Link. -->
              <!-- <span class="link-block">
                <a href="https://mathverse-cuhk.github.io/#visualization"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üîÆ</p>
                  </span>
                  <span>Visualize</span>
                </a>
              </span> -->
              <!-- Leaderboard Link. -->
              <!-- <span class="link-block">
                <a href="https://mathverse-cuhk.github.io/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>In recent years, Multimodal Large Language Models(MLLMs) have increasingly emphasized grounding and referring capabilities to achieve detailed understanding and flexible user interaction. However, in the realm of visual document understanding, these capabilities lag behind due to the scarcity of fine-grained datasets and comprehensive benchmarks. 
          </p>
          <p>  
            To fill this gap, we propose the <b>DO</b>cument <b>G</b>rounding and r<b>E</b>ferring data engine (<img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><b>DOGE-Engine</b>), which produces two types of high-quality fine-grained document data: multi-granular parsing data for enhancing fundamental text localization and recognition capabilities; and
            instruction-tuning data to activate MLLM‚Äôs grounding and referring capabilities during dialogue and reasoning. Additionally, using our engine, we construct <img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><b>DOGE-Bench</b>, which encompasses 7 grounding and referring tasks across 3 document types (chart, poster, PDF document), providing
            comprehensive evaluations for fine-grained document understanding. Furthermore, leveraging the data generated by our engine, we develop a strong baseline model, <img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><b>DOGE</b>.
            This pioneering MLLM is capable of accurately referring and grounding texts at multiple granularities within document images. Our code, data, and model will be opensourced for community development.
          </p>
          
        </div>
      </div>
    </div>
</div>
</section>


<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <img src="static/images/doge.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="doge" style="vertical-align: middle">DOGE-Engine</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Well-annotated and diverse grounded data are crucial for improving the grounding and referring capabilities of MLLMs. Currently, there is a shortage of comprehensive and accurately labeled document grounded data. 
            Manually annotating raw document images is both time-consuming and labor-intensive, as it requires not only marking the bounding boxes but also accurately annotating all the text within those boxes. 
            To tackle this challenge, we collect a substantial volume of documents and develop the <img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">DOGE-Engine</span> to construct fine-grained document grounded datasets. Our document data sources primarily encompass three document data types:
            posters, charts, and PDF documents. As shown below, we start by filtering the raw data to remove low-quality samples or those with missing or broken information. An overview of training data statistical information and two strategies are also provided below.

          </p>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/pipeline.png" alt="algebraic reasoning" width="100%"/>
              <p> Pipeline of
                <span class="doge">DOGE-Engine</span>.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/doge_data.png" alt="arithmetic reasoning" width="60%"/>
              <p> Training data composition of 
                <span class="doge">DOGE-Dataset</span>.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/bitwise2.png" alt="arithmetic reasoning" width="100%"/>
              <p> <b>Rerendering Stategy</b> for Poster and Chart.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/merge.png" alt="arithmetic reasoning" width="100%"/>
              <p> <b>Merge Strategy</b> for PDF document.</p>
            </div>
          </div>
        </div>
      </div> 


          <p>
            <!-- TODO -->
            You can download the dataset on <a href="https://github.com/zyinan99/DOGE" target="_blank">Hugging Face Dataset</a>.
          </p>

        </div>
      </div>
    </div>


<!-- Bench SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista">
    <img src="static/images/doge.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="doge" style="vertical-align: middle">DOGE-Bench & DOGE</span>
  </h1>
  </div>
</section>
    <div class="columns is-centered">
      <div class="column" style="margin-right: -20rem;">
        <div class="content has-text-centered">
          <img src="static/images/bench_construct2.png" alt="data-overview" style="max-width: 50%;"/>
          <p> 
            7-Task Definition of <img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="doge">DOGE-Bench</span>.<br/>
          </p> 
        </div>
        <div class="content has-text-centered">
          <img src="static/images/statistics_bench.png" alt="data-overview" style="max-width: 50%;"/>
          <p> 
            Data statistic of <img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="doge">DOGE-Bench</span>.<br/>
          </p> 
        </div>
      </div>
      <div class="column">
        <div class="content has-text-centered">
          <img src="static/images/model.png" alt="data-composition" style="max-width: 56%;"/>
          <p>
              Model architecture of <img src="static/images/doge.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="doge">DOGE</span>.
          </p>
        </div>
      </div>
    </div>

    

<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Experiment Results</h1>
  </div>
</section>

<section class="section">
  <div class="container">

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <!-- <h2 class="title is-3">Results on Existing General Document Benchmarks</h2> -->
        <!-- <p>One example for each reasoning skill required in <span class="mathvista">MathVista</span></p> -->
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/general.png" alt="grade-lv" width="60%"/>
              <p>Results on Existing General Document Benchmarks.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/doge_result.png" alt="grade-lv" width="80%"/>
              <p>Results of DOGE on DOGE-Bench.</p>
            </div>
          </div>
          
        </div>
      </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Visualization Examples</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/image/quantitative2.png" alt="" width="80%"/>
              <p>Response Comparison of GPT-4V, LLaVA-NeXT, and SPHINX-MoE. We adopt the Text-lite version of the problem, and highlight the key-step extraction and scoring by the CoT evaluation strategy.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/image/sample_indomain.png" alt="" width="80%"/>
              <p>Response Comparison of GPT-4V, LLaVA-NeXT, and SPHINX-MoE. We adopt the Text-lite version of the problem, and highlight the key-step extraction and scoring by the CoT evaluation strategy.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/image/sample_outdomain.png" alt="" width="80%"/>
              <p>Response Comparison of GPT-4V, LLaVA-NeXT, and SPHINX-MoE. We adopt the Text-lite version of the problem, and highlight the key-step extraction and scoring by the CoT evaluation strategy.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/image/sample_failure.png" alt="" width="80%"/>
              <p>Response Comparison of GPT-4V, LLaVA-NeXT, and SPHINX-MoE. We adopt the Text-lite version of the problem, and highlight the key-step extraction and scoring by the CoT evaluation strategy.</p>
            </div>
          </div>


        </div>
      </div>
    </div>

  </div>
</section>


<!-- @PAN TODO: bibtex -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}</code></pre>
  </div>
</section> -->




<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://mathvista.github.io/">MathVista</a> and <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
